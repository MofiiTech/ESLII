{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Variable Types and Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming convention for the prediction tasks:\n",
    "- **regression** when we predict quantitative outputs\n",
    "- **classification** when we predict qualitative outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Linear Models and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a vector of inputs $X^T = (X_1, X_2, ..., X_p)$, we predict the output $Y$ via the model\n",
    "\n",
    "$$\\hat{Y} = \\hat{\\beta_0} + \\sum_{j=1}^{p}X_j \\hat{\\beta_j}$$\n",
    "\n",
    "The term $\\hat{\\beta_0}$ is the intercept, also known as the **bias**. Often it is convenient to include the constant variable $1$ in $X$, include $\\hat{\\beta_0}$ in the vector $\\hat{\\beta}$, and then write the linear model in vector form as an inner product\n",
    "\n",
    "$$\\hat{Y} = X^T\\hat{\\beta}$$\n",
    "\n",
    "The most popular method to fit the linear model to the set of training data is the method of **least squares**. In this approach, we pick the coefficients $\\beta$ to minimize the residual sum of squares\n",
    "\n",
    "$$\\operatorname{RSS}(\\beta) = \\sum_{i=1}^{N} (y_i - x_i^T\\beta)^2$$\n",
    "\n",
    "The solution is easiest to characterize in matrix notation. We can write\n",
    "\n",
    "$$\\operatorname{RSS}(\\beta) = (\\mathbf{y} - \\mathbf{X}\\beta)^T(\\mathbf{y} - \\mathbf{X}\\beta)$$\n",
    "\n",
    "Differentiating w.r.t. $\\beta$ we get the **normal equations**\n",
    "\n",
    "$$\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\beta) = 0$$\n",
    "\n",
    "If $\\mathbf{X}^T\\mathbf{X}$ is nonsingular, then the unique solution is given by\n",
    "\n",
    "$$\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "Intuitively, it seems that we do not need a very large data set to fit such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the linear model is fit on the data, there are several misclassifications on both side of the decision boundary. Consider the two possible scenarios:\n",
    "- **Scenario 1**: The training data in each class was generated from bivariate Gaussian distributions with uncorrelated components and different menas.\n",
    "- **Scenario 2**: The training data in each class came from a mixture of 10 low-variance Gaussian distributions, with individual means themselves distributed as Gaussian.\n",
    "\n",
    "In the case of one Gaussian per class, we will see in Chapter 4 that a linear decision boundary is the best one can do, and that our estimate is almost optimal. In the case of mixtures of tightly clustered Gaussian, a linear decision boundary is unlikely to be optimal, and in fact is not. The optimal decision boundary is nonlinear and disjoint, and as such will be much more difficult to obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Nearest-Neighbor Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest-neighbor methods use those obervations in the training set $\\mathcal{T}$ closest in input space to $x$ to form $\\hat{Y}$. Specifically, the $k$-nearest neighbor fit for $\\hat{Y}$ is defined as\n",
    "\n",
    "$$\\hat{Y}(x) = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i$$\n",
    "\n",
    "where $N_k(x)$ is the neighborhood of $x$ defined by the $k$ closest points $x_i$ in the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
